---
title: A Working Manifesto
date: 2026-01-16
tags:
  - reflection
  - AI
  - psychoanalysis
published: true
---
I only began working consistently with AI-assisted programming after 2026. What caught me off guard wasn’t how much faster things became, but how quickly the work started to feel heavier in a different way. There’s a quiet assumption circulating right now—rarely stated outright—that because AI systems don’t seem to have obvious limits, our work should begin to resemble that limitlessness as well. But whatever AI can do, we remain embodied. We still get tired. Our attention frays. Our bodies register cost long before our tools do. The fact that something can now be done doesn’t automatically mean it needs to be done, or done immediately, or done by us at all.

It took me some time to notice that “low cost” doesn’t mean “no cost.” Time, emotional bandwidth, and the ability to stay oriented toward a longer process are still being spent, even when the interaction feels frictionless. In practice, AI didn’t simply expand my capacity; it made its edges more visible. It kept pushing me back toward a question that existed long before these tools did: what was I actually trying to do in the first place? That question doesn’t disappear when new technology arrives. If anything, it becomes easier to bypass. AI doesn’t generate intention. It amplifies movement once intention is already there—and sometimes it amplifies movement that shouldn’t have been happening to begin with.

At some point, I realized that what AI often helps with isn’t purely technical. Very often, it’s emotional. From a psychoanalytic perspective, the inability to think is rarely just a cognitive problem. It’s tied to anxiety, overload, and the experience of being stuck inside something that refuses to take shape. In that sense, when AI helps me think, it isn’t because it “knows more,” but because it temporarily holds something for me. It acts as a container, a place where half-formed ideas can exist without immediately collapsing into frustration. That doesn’t mean it thinks on my behalf. But it can make thinking possible again when it wasn’t.

Working at this speed also stripped away certain excuses. When something didn’t work—when a system felt incoherent, or an application kept growing without direction—it was rarely because the AI failed to produce a solution. More often, it was because the design itself was confused. In game development especially, there were moments where the AI produced exactly what I asked for, and that precision only made the underlying problem clearer. The issue wasn’t implementation. It was that I hadn’t thought carefully enough about what the system was meant to do, or what kind of experience it was supposed to hold. In that sense, AI didn’t compensate for my blind spots as a designer. It exposed them.

I’ve heard programmers ask why working with AI sometimes feels more exhausting, not less. I think part of the answer lies in constant cognitive recalibration. You’re no longer just solving problems; you’re repeatedly expanding the frame in which problems appear. What counts as possible shifts, then shifts again. That adjustment takes energy, even when it looks like productivity from the outside. Without some kind of continuous narrative, the work starts to feel scattered. This is part of why I felt the need to write this blog—not to explain AI, but to leave myself some footsteps. A consistent narrative doesn’t remove fatigue, but it can contain it. It gives thinking somewhere to land.

This is also how I’ve come to understand my collaboration with AI more broadly, whether I’m working on games, artistic projects, or small tools. The question for me isn’t how to make AI more powerful, but how to bring ideas from psychoanalysis—containment, pacing, tolerance for uncertainty—into the way things are designed and built. I’ve been using the internal name *Inner Tools* for this line of work, not as a claim about authenticity, but as a reminder of what tends to get excluded. When I look at many contemporary applications, the gaps are often emotional rather than technical. Design assumptions still orbit around efficiency, engagement, and growth. Very little attention is paid to how a tool feels to stay with, or what kind of inner state it quietly rewards.

Another shift I didn’t fully anticipate was how AI changed my relationship to my own different identities. For a long time, carrying multiple roles—Buddist, psychoanalyst, writer, artist, game developer—felt more destabilizing than generative. Each field seemed to demand a level of professionalism that made everything else feel insufficient. What AI quietly altered was that pressure. Outside of my primary professional commitment to psychoanalysis, I no longer felt the need to become fully qualified in every adjacent domain before allowing myself to explore it. AI made it possible to work seriously without being exhaustive, to develop ideas without first earning permission through mastery.

This mattered in very practical terms. Between heavy clinical work and a doctoral program, there simply wasn’t enough time or energy left to sustain multiple lines of experimentation under older conditions. Many of these explorations would have remained ideas I thought about but never touched. AI didn’t remove those constraints, but it eased them just enough that trying became possible again. In that sense, it functioned less as an accelerator than as an encouragement—an encouragement toward amateurism, toward crossing boundaries, toward working at the edges of fields rather than deep inside a single one. For me, those crossings have always been where something new begins, not through depth alone, but through connection.

One thing AI has shifted, at least potentially, is who gets to make things at all. When production costs drop and development becomes less dependent on large teams or guaranteed markets, it becomes possible to build tools that don’t serve a mass audience or a manufactured demand. Some ideas don’t scale well. They don’t promise profit, only care, specificity, or relief for a small number of people. Under earlier conditions, those ideas would rarely survive long enough to be built. AI doesn’t solve this problem, but it lowers the threshold just enough that these kinds of projects can exist without having to justify themselves economically from the start.

On a more personal level, this way of working has also felt like a return to something I missed earlier. Collaborating with AI has brought me back to a kind of play I didn’t really get to inhabit as a teenager—the pleasure of making things without a clear outcome, of following curiosity without needing it to turn into a product. That playfulness doesn’t replace seriousness or discipline, but it changes their tone. It reminds me that creation doesn’t always need to be optimized to be valid. Sometimes it just needs enough space to be enjoyed while it’s happening.

This blog is part of that space. Not a record of best practices, but a place to think alongside tools without being absorbed by them. Not a claim about the future of AI, but a way of staying oriented while working with it. If there’s a position here, it’s a modest one: technology matters not only because of what it makes possible, but because of what it asks us to hold, to refuse, and to return to—again and again—when the limits are no longer imposed from the outside.
